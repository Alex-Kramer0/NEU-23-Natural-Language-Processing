{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOybUIgqgdok"
   },
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8xHsfLOgdon"
   },
   "source": [
    "Task 3: Feedforward Neural Language Model (60 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnOIDn6dgdoo"
   },
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ziz2hzSRgdop",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 11:23:06.703485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexkramer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing utility functions from Keras\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# necessary\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# optional\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm import notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# your other imports here\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import neurallm_utils as nutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uKbuYQmpgdor",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3 # The ngram language model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCgssmuPioXJ",
    "outputId": "d058a13f-c4eb-43dc-86c4-f1b1009426f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in necessary data\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "data_word = nutils.read_file_spooky(\"spooky_author_train.csv\", NGRAM, by_character=False)\n",
    "data_char = nutils.read_file_spooky(\"spooky_author_train.csv\", NGRAM, by_character=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N-amLekEgdos",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "# do this for both the word and character data\n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from\n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "\n",
    "# CHARACTERS\n",
    "tokenizer_char = Tokenizer()\n",
    "tokenizer_char.fit_on_texts(data_char)\n",
    "encoded_char = tokenizer_char.texts_to_sequences(data_char)\n",
    "\n",
    "# WORDS\n",
    "tokenizer_word = Tokenizer()\n",
    "tokenizer_word.fit_on_texts(data_word)\n",
    "encoded_word = tokenizer_word.texts_to_sequences(data_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCggMc1ngdot",
    "outputId": "d7c0c550-6b40-4cb5-f8cc-a2b6bcd9c458",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 60\n",
      "Words: 25374\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "\n",
    "\n",
    "print(\"Characters:\",len(tokenizer_char.index_word))\n",
    "print(\"Words:\",len(tokenizer_word.index_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bffdBw3vgdov"
   },
   "source": [
    "### b) Next, prepare the sequences to train your model from text (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_6z_PAygdow"
   },
   "source": [
    "#### Fixed n-gram based sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "LirUTU_Hgdox"
   },
   "source": [
    "The training samples will be structured in the following format.\n",
    "Depening on which ngram model we choose, there will be (n-1) tokens\n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process                                    however\n",
    "    process, however                                    afforded\n",
    "    however, afforded\t                                me\n",
    "\n",
    "\n",
    "Our first step is to translate the text into sequences of numbers,\n",
    "one sequence per n-gram window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nzSvktHgdoy",
    "outputId": "a8f64544-f168-4ea6-bd1d-eec644590837",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  [[1, 1, 32], [1, 32, 2956], [32, 2956, 3], [2956, 3, 155], [3, 155, 3]]\n",
      "634080\n",
      "Characters:  [[21, 21, 3], [21, 3, 9], [3, 9, 7], [9, 7, 8], [7, 8, 1]]\n",
      "2957553\n"
     ]
    }
   ],
   "source": [
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "\n",
    "def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and\n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return:\n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "\n",
    "    ngrams = []\n",
    "    for text in encoded:\n",
    "        for i in range(len(text)-ngram+1):\n",
    "            ngrams.append([text[x] for x in range(i, i+ngram)])\n",
    "    return ngrams\n",
    "\n",
    "# WORDS\n",
    "ngrams3_word = generate_ngram_training_samples(encoded_word, 3)\n",
    "\n",
    "#CHARACTERS\n",
    "ngrams3_char = generate_ngram_training_samples(encoded_char, 3)\n",
    "\n",
    "\n",
    "# WORDS\n",
    "print('Words: ', ngrams3_word[:5])\n",
    "print(len(ngrams3_word))\n",
    "\n",
    "#CHARACTERS\n",
    "print('Characters: ', ngrams3_char[:5])\n",
    "print(len(ngrams3_char))\n",
    "\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "\n",
    "# Spooky data by words shoud give 634080 sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOXQthoCgdoz"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxTrWnacgdoz",
    "outputId": "c94e8ffc-dddc-4e0f-9cff-4049d1fd7adf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634080 634080\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Note here that the sequences were in the form:\n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here\n",
    "\n",
    "# RUN THIS FOR WORDS\n",
    "X, y = [], []\n",
    "for item in ngrams3_word:\n",
    "    X += [item[:-1]]\n",
    "    y += [item[-1]]\n",
    "\n",
    "# print out the shapes to verify that they are correct\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS FOR CHARACTERS\n",
    "X, y = [], []\n",
    "for item in ngrams3_char:\n",
    "    X += [item[:-1]]\n",
    "    y += [item[-1]]\n",
    "\n",
    "# print out the shapes to verify that they are correct\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kwqwmASIgdo0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "# Initialize a function that reads the word embeddings you saved earlier\n",
    "# and gives you back mappings from words to their embeddings and also\n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "# the \"0\" index of the Tokenizer is assigned for the padding token. Initialize\n",
    "# the vector for padding token as all zeros of embedding size\n",
    "# this adds one to the number of embeddings that were initially saved\n",
    "# (and increases your vocab size by 1)\n",
    "\n",
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    word_dict = dict() # also works for characters\n",
    "    index_dict = dict()\n",
    "    with open(filename, \"r\") as embeddings_file:\n",
    "        dimensions = embeddings_file.readline()\n",
    "        index_dict[0] = [0.0] * int(dimensions.split(' ')[1])\n",
    "        for line in embeddings_file.readlines():\n",
    "            key_word = line.split(' ')[0]\n",
    "            key_index = tokenizer.word_index[key_word]\n",
    "            value = [float(x) for x in line.split(' ')[1:]]\n",
    "\n",
    "            word_dict[key_word] = value\n",
    "            index_dict[key_index] = value\n",
    "    return word_dict, index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fD4RIPstgdo1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10 points\n",
    "## WE NEED THE OUTPUT TO BE X, y\n",
    "# X is an arrayed list of everything in the batch concatenated!!!\n",
    "# y is an array of the one-hot encoded\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict, tokenizer, for_feedforward: bool = False) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "\n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels\n",
    "    (see the to_categorical function)\n",
    "\n",
    "    If for_feedforward is True:\n",
    "    Returns data generator to be used by feed_forward\n",
    "    else: Returns data generator for RNN model\n",
    "    '''\n",
    "    while True:\n",
    "        num_classes = len(tokenizer.index_word)+1\n",
    "        for start in range(0,len(X),num_sequences_per_batch):\n",
    "            batch_X,batch_y = [],[]\n",
    "            i = start\n",
    "            #for i in range(start,start+num_sequences_per_batch):\n",
    "            while i < start+num_sequences_per_batch and i < len(X):\n",
    "                sequence = []\n",
    "                for word in X[i]:\n",
    "                    sequence += index_2_embedding[word]\n",
    "                batch_X += [sequence]\n",
    "                batch_y += [to_categorical(y[i], num_classes=num_classes)] # label one hot encoded\n",
    "                #if for_feedforward:\n",
    "                #    batch += [sequence]#[(sequence, y[start:start+num_sequences_per_batch])]\n",
    "                i+=1\n",
    "            yield np.array(batch_X), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0eiFnTjCgdo2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5 points\n",
    "\n",
    "# initialize your data_generator for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is correct for both word and character data\n",
    "\n",
    "## RUN THIS FOR WORDS\n",
    "word_2_embeddings, index_2_embeddings = read_embeddings(\"spooky_embedding_word.txt\", tokenizer_word)\n",
    "\n",
    "# Examples:\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "steps_per_epoch = len(X)//num_sequences_per_batch  # Number of batches per epoch # sequences > X\n",
    "train_generator = data_generator(X, y, num_sequences_per_batch, index_2_embeddings, tokenizer_word, for_feedforward=True)\n",
    "\n",
    "#sample=next(train_generator) # this is how you get data out of generators\n",
    "#sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "#sample[1].shape   # (batch_size, |V|) to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS FOR CHARACTERS\n",
    "word_2_embeddings, index_2_embeddings = read_embeddings(\"spooky_embedding_char.txt\", tokenizer_char)\n",
    "\n",
    "\n",
    "# Examples:\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "steps_per_epoch = len(X)//num_sequences_per_batch  # Number of batches per epoch # sequences > X\n",
    "train_generator = data_generator(X, y, num_sequences_per_batch, index_2_embeddings, tokenizer_char, for_feedforward=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzwgInR6gdo3"
   },
   "source": [
    "### d) Train & __save__ your models (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "cjNBt2_7gdo3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 15 points\n",
    "\n",
    "# code to train a feedforward neural language model for\n",
    "# both word embeddings and character embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "# (define functions as needed)\n",
    "\n",
    "#5ep H\n",
    "model = Sequential(\n",
    "[Dense(100, activation=\"exponential\", name=\"1\"), # relu > softmax\n",
    " Dense(100, activation=\"exponential\", name=\"2\"), # relu > softmax\n",
    " Dense(len(tokenizer_word.word_index)+1, activation ='sigmoid', name=\"3\")])\n",
    "\n",
    "\n",
    "# layers.Activation('softmax')\n",
    "\n",
    "loss_fn = 'categorical_crossentropy'\n",
    "model.compile(\n",
    "    loss=loss_fn,\n",
    "    optimizer='adam',\n",
    "    metrics=[keras.metrics.Accuracy()])\n",
    "\n",
    "\n",
    "# train your models for between 3 & 5 epochs\n",
    "# on Felix's machine, this takes ~ 24 min for character embeddings and ~ 10 min for word embeddings\n",
    "# DO NOT EXPECT ACCURACIES OVER 0.5 (and even that is very for this many epochs)\n",
    "# We recommend starting by training for 1 epoch\n",
    "\n",
    "# Define your model architecture using Keras Sequential API\n",
    "# Use the adam optimizer instead of sgd\n",
    "# add cells as desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING \n",
    "\"\"\"\n",
    "model = Sequential(\n",
    "[Dense(100, activation=\"exponential\", name=\"1\"),\n",
    " Dense(100, activation=\"exponential\", name=\"2\"),\n",
    " Dense(len(tokenizer.word_index)+1, activation =\"sigmoid\", name=\"3\")]\n",
    ")\n",
    "#5ep G\n",
    "model = Sequential(\n",
    "[Dense(100, activation=\"exponential\", name=\"1\"), # relu > softmax\n",
    " Dense(160, activation=\"exponential\", name=\"2\"), # relu > softmax\n",
    " Dense(len(tokenizer.word_index)+1, activation ='sigmoid', name=\"3\")])\n",
    "\n",
    "#5ep e\n",
    "model = Sequential(\n",
    "[Dense(100, activation=\"relu\", name=\"1\"), # relu > softmax\n",
    " Dense(160, activation=\"relu\", name=\"2\"), # relu > softmax\n",
    " Dense(len(tokenizer.word_index)+1, activation ='sigmoid', name=\"3\")])\n",
    "#sigmoid >  > relu\n",
    "\n",
    "#5ep F\n",
    "model = Sequential(\n",
    "[Dense(100, activation=\"selu\", name=\"1\"), # relu > softmax\n",
    " Dense(160, activation=\"selu\", name=\"2\"), # relu > softmax\n",
    " Dense(len(tokenizer.word_index)+1, activation ='sigmoid', name=\"3\")])\n",
    "Epoch 1/2\n",
    "4953/4953 [==============================] - 453s 91ms/step - loss: 5.7491 - accuracy: 1.3446e-07\n",
    "Epoch 2/2\n",
    "4953/4953 [==============================] - 456s 92ms/step - loss: 5.3615 - accuracy: 1.5243e-06\n",
    "\n",
    "#5ep E accuracy: 0.0285\n",
    "model = Sequential(\n",
    "[Dense(100, activation=\"relu\", name=\"1\"), # relu > softmax\n",
    " Dense(160, activation=\"relu\", name=\"2\"), # relu > softmax\n",
    " Dense(len(tokenizer.word_index)+1, activation ='sigmoid', name=\"3\")])\n",
    "\n",
    "#5ep A\n",
    "model = Sequential(\n",
    "[Dense(128*200, activation=\"relu\", name=\"1\"), # relu > softmax\n",
    " Dense(160, activation=\"relu\", name=\"2\"), # relu > softmax\n",
    " Dense(len(tokenizer.word_index)+1, activation ='sigmoid', name=\"3\")]\n",
    ") -> loss: 4.9513 - accuracy: 0.0329 @epoch 5\n",
    "\n",
    "#5ep D\n",
    "model = Sequential(\n",
    "[Dense(128*200, activation=\"relu\", name=\"1\"), # relu > softmax\n",
    " Dense(200, activation=\"relu\", name=\"2\"), # relu > softmax\n",
    " Dense(200, activation=\"relu\", name=\"3\"), # relu > softmax\n",
    " Dense(len(tokenizer.word_index)+1, activation ='sigmoid', name=\"4\")]\n",
    ")\n",
    "# loss: 4.8354 - accuracy: 0.0300\n",
    "\n",
    "#5ep C\n",
    "model = Sequential(\n",
    "[Dense(128*200, activation=\"relu\", name=\"1\"), # relu > softmax\n",
    " Dense(180, activation=\"relu\", name=\"2\"), # relu > softmax\n",
    " Dense(160, activation=\"relu\", name=\"3\"), # relu > softmax\n",
    " Dense(len(tokenizer.word_index)+1, activation ='sigmoid', name=\"4\")]\n",
    ") -> loss: 4.8550 - accuracy: 0.0312\n",
    "\n",
    "#5ep B\n",
    "model = Sequential(\n",
    "[Dense(128*200, activation=\"relu\", name=\"1\"), # relu > softmax\n",
    " Dense(180, activation=\"relu\", name=\"2\"), # relu > softmax\n",
    " Dense(len(tokenizer.word_index)+1, activation ='sigmoid', name=\"3\")]\n",
    ") -> loss: 4.9408 - accuracy: 0.0022\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XnA1isotgdo4",
    "outputId": "35a7f6a8-5133-455f-d151-2972b599b172",
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is some example code to train a model with a data generator\n",
    "model.fit(x=train_generator,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          batch_size=num_sequences_per_batch,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgzZntKigdo4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spooky data model by character for 5 epochs takes ~ 24 min on Felix's computer\n",
    "# with adam optimizer, gets accuracy of 0.3920\n",
    "\n",
    "\n",
    "\n",
    "# spooky data model by word for 5 epochs takes 10 min on Felix's computer\n",
    "# results in accuracy of 0.2110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_K_CxZngdo5"
   },
   "outputs": [],
   "source": [
    "# save your trained models so you can re-load instead of re-training each time\n",
    "# also, you'll need these to generate your sentences!\n",
    "filepath = 'word_FFNN_3epG.keras' \n",
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FOR CHARACTERS\n",
    "model = Sequential(\n",
    "[Dense(100, activation=\"exponential\", name=\"1\"), # relu > softmax\n",
    " Dense(225, activation=\"relu\", name=\"2\"), # relu > softmax\n",
    " Dense(len(tokenizer_char.word_index) + 1, activation ='sigmoid', name=\"3\")])\n",
    "\n",
    "loss_fn = 'categorical_crossentropy'\n",
    "model.compile(\n",
    "    loss=loss_fn,\n",
    "    optimizer='adam',\n",
    "    metrics=[keras.metrics.Accuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN FOR CHARACTERS\n",
    "model.fit(x=train_generator,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          batch_size=num_sequences_per_batch,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FOR CHARACTERS\n",
    "filepath = 'char_FFNN_3epG.keras' \n",
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5xdkDnmgdo5"
   },
   "source": [
    "### e) Generate Sentences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4zCM8bxgdo6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load your models if you need to\n",
    "tf.keras.saving.load_model(\n",
    "    filepath, custom_objects=None, compile=True, safe_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJyUBi4xgdo6"
   },
   "outputs": [],
   "source": [
    "# 10 points\n",
    "# # generate a sequence from the model until you get an end of sentence token\n",
    "import random\n",
    "def generate_seq(model: Sequential, tokenizer: Tokenizer, word_2_embedding, seed: list, verbose: bool = False, words: bool = True, max_len: int = 100, binary: bool=True):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        word_2_embedding: the word to embedding dict\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    sentence = [word for word in seed]\n",
    "    while sentence[-1] not in [\"</s>\", \"\"] and len(sentence) <= max_len:\n",
    "        X = []\n",
    "        for word in sentence[-2:]:\n",
    "            X += word_2_embeddings[word]\n",
    "        next_vector = model.predict(np.array([X]), verbose=verbose)[0] # this comes out as an array \n",
    "        if binary:\n",
    "            best = 0.01\n",
    "        else:\n",
    "            best = max(next_vector)\n",
    "        next_token = []\n",
    "        for i in range(0,len(tokenizer.index_word)+1):\n",
    "            if next_vector[i] >= best:\n",
    "                if i == 0:\n",
    "                    next_token.append('')\n",
    "                else:\n",
    "                    next_token.append(tokenizer.index_word[i])\n",
    "        if len(next_token) == 0:\n",
    "            break\n",
    "        else:\n",
    "            sentence += [random.choice(next_token)]\n",
    "        if verbose and words:\n",
    "            print(' '.join(sentence), end=\"/r\")\n",
    "        elif verbose:\n",
    "            print(''.join(sentence), end=\"/r\")\n",
    "    return format_sentence(sentence, words)\n",
    "\n",
    "\n",
    "def format_sentence(sentence: list, words: bool = True):\n",
    "    if sentence[0] == \"<s>\":\n",
    "        sentence = sentence[1:]\n",
    "    if sentence[-1] == \"</s>\":\n",
    "        sentence = sentence[:-1]\n",
    "    if words:\n",
    "        sentence = [' '+word for word in sentence if word not in '.,!?;']\n",
    "    else:\n",
    "        sentence = [letter.replace('_', ' ') for letter in sentence]\n",
    "    return ''.join(sentence).strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_results = generate_seq(model, tokenizer_word, word_2_embeddings, ['<s>','earth'], verbose=True, words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_results = generate_seq(model, tokenizer_char, word_2_embeddings, ['<s>','t'], verbose=True, words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5ksBo_Hgdo6"
   },
   "outputs": [],
   "source": [
    "# 5 points\n",
    "print(\"Sentence generated by words: \\n\", word_results, '\\n\\n')\n",
    "\n",
    "print(\"Sentence generated by characters: \\n\", char_results)\n",
    "# generate and display one sequence from both the word model and the character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# you may leave _ as _ or replace it with a space if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_liQc_KFgdo6"
   },
   "outputs": [],
   "source": [
    "# generate 100 example sentences with each model and save them to a file, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model\n",
    "def generate_multiple(n: int, filename: str, model, tokenizer, word_2_embedding, words: bool = True): # seeds: list,\n",
    "    sentences = \"\"\n",
    "    while n > 0:\n",
    "        try:\n",
    "            seed = ['<s>'] + [random.choice(list(tokenizer.word_index))]\n",
    "            seq = generate_seq(model, tokenizer, word_2_embedding, seed, verbose=False, words=words)\n",
    "            sentences += seq + '\\n'\n",
    "            n -= 1\n",
    "            print(seq, end='\\n\\n\\r')\n",
    "        except:\n",
    "            pass\n",
    "        with open(filename, 'w') as outputfile:\n",
    "            outputfile.write(sentences[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q34biJLgdo7"
   },
   "outputs": [],
   "source": [
    "generate_multiple(100, \"100_sentences_word.txt\", model, tokenizer, word_2_embeddings, words=True) \n",
    "# Alex fix me --> change tokenizer to tokenizer_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_multiple(20, \"100_sentences_word.txt\", model, tokenizer, word_2_embeddings, words=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_multiple(100, \"100_sentences_char.txt\", model, tokenizer_char, word_2_embeddings, words=False) \n",
    "# Alex fix me --> change tokenizer to tokenizer_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
